% vim: set tw=78 sts=2 sw=2 ts=8 aw et ai:

\subsection{Datasets Used}

We have used the Caltech Cars (Rear) dataset. There are 2 datasets. First contains 1155 images of cars from the rear. It was used at CVPR '03. All cars are centered inside the images, and there is no need for scanning. Also, all images have the same height and width. So, we used this set only for classification tasks. We achieved a 97 \% F1-score for 10 - fold cross validation.

The next dataset we focued is the Caltech Cars(Rear) 2 Dataset. This dataset contains 126 images that contain cars shot from behind. The cars are not centered, and there is no annotation with the car position provided.

We also emplyed a total of 200 negative images, that don't contain cars.

\subsection{Cropping the Cars}

The first step in building a robust classifier was to provide it with adequate training iamges. So, We build a script that permitted to interactively extract the car from the images provided. Besides the cropped image with the car, information about the coordinates of the bounding rectangle was stored. This information was used as a ground truth when we estimated the detection performance of the system.

We focused our attention on 2 tasks:

\begin{itemize}
\item Classification - Does the image contain a car, or not?
\item Detection - Where is located the car in the image, if any?
\end{itemize}

\subsection{Classification Task}

For the classification task, we worked only on cropped images, and on negative images (background). The image was taken as a whole. The question we had to answer was: Is it a car or not in the image?

As depicted in the pictures below, this is how the system would work for a given image:

\begin{figure}[htb]
	\begin{center}
		\includegraphics[scale=0.6]{img/result35.jpg}
	    \caption{Correct Classification \label{img:result35}}
    \end{center}
\end{figure}

\begin{figure}[htb]
	\begin{center}
		\includegraphics[scale=0.6]{img/result8.jpg}
	    \caption{Incorrect Classification \label{img:result8}}
    \end{center}
\end{figure}

We used 2 methods of estimating the performance of the system:

\begin{itemize}
\item K - fold cross validation with K = 10
\item Splitting the set in training/test set with 60\% traingin set, 40\% test set
\end{itemize}

Initially, we have chosen for K-fold cross validation a setup of 4 height (vertical) cells and 8 horizontal cells. The F1-score determined for this setup was high, of 97\%.

Then we varied the height cells count form 2 to 10, and the horizontal cells count form 4 to 14. We chose these ranges because because cars pictured from behind span more on horizontal axis than on vertical axis.

On a 60\% training 40\% testing, we obtained very high (>97\%) F1 - scores on all horizontal / height cells counts combinations. Best accuracy was achieved at 4 height cells (vertical cells), and 10 horizontal cells (width cells).  Accuracy was 99.69\%. It is a remarkably high accuracy.

The dependency of accuracy is depicted in the surface plot below:

\begin{figure}[htb]
	\begin{center}
		\includegraphics[scale=0.27]{img/accuracy-hcellcount-vcellcount.jpg}
	    \caption{Surface plot depicting F1-score for different cell counts \label{img:accuracy-hcellcount-vcellcount}}
    \end{center}
\end{figure}

Keeping constant either the optimum horizontal cell count (10), or the optimum vertical cell count(4) we can draw the following plots for the other dimension:


\begin{figure}[htb]
	\begin{center}
		\includegraphics[scale=0.23]{img/accuracy-horizontal-cell-count-4.jpg}
	    \caption{Horizontal cell count dependence for optimum vertical cell count \label{img:accuracy-horizontal-cell-count-4}}
    \end{center}
\end{figure}

\begin{figure}[htb]
	\begin{center}
		\includegraphics[scale=0.25]{img/accuracy-vertical-cell-count-10.jpg}
	    \caption{Vertical cell count dependence for optimum horizontal cell count \label{img:accuracy-vertical-cell-count-10}}
    \end{center}
\end{figure}

\subsection{Detection Task}

For the detection taks, we not only have to answer the question "Does the image contain a car?", but we have to locate, to tell where the car is located inside the image.

Our method used as a training set the cropped images with cars. Then, we trained on 70\% of these cropeed images with cars, and we kept the other 30\% of images as unseen examples. We selected the corresponding original images from the remaining 30\% of the unseen cars, and tested the performance of the algorithm on this set.

Our method of scanning is rather simple. We scan the image with a moving window, at different scales. For each window, we compute the HOG with the corresponding number of horizontal and vertical cells. Then we feed this HOG to the SVM classifier.

We scanned at different scales, namley 3 scales: 1, 1/2, 1/3, and used a sliding window with increments of 10 pixels. To detect the entire car, we used a voting system. Whenever the SVM classifier detects a car, the number of votes of each pixel inside that window is increased by 1.

\begin{figure}[htb]
	\begin{center}
		\includegraphics[scale=0.48]{img/result73.jpg}
	    \caption{Every green rectangle is a prediction of the car.\label{img:result73}}
    \end{center}
\end{figure}

Without voting, many such windows would trigger the detection of a car, so the idea of voting came naturally. This can be seen in the following image. So, every such window voted, and at the end of the day the coordinates of the pixel with the highest number of votes was chosen as the car center. It there was a tie, e.g. many maximum points, the first one was chosen.



Then, starting from this point, we extended our car detection rectangle outwards. We moved on vertical and horizontal axis and checked each pixel votes. We stopped when the pixel vote declined under 0.3 of maximum number of votes. The bounding box of the detected car was also chosen.

In order to estimate the performance, a detection was said to be correct when the amount of overlap between the detected rectangle and the ground truth rectangle is greater than some trheshold. Typically, a threshold of 50\% is chosen.

We compute overlap fraction this way. We compute intersection and union of the detection rectangle and of the ground truth rectangle, obtained when the car was cropped.


\begin{equation}
Overlap = \frac{Intersection}{Union}
\end{equation}

In the image below, the green rectangle denotes the detection, and the blue rectangle denotes the real car, as ground truth. In the top left corner the overlap is shown.

We also varied the number of vertical cells and horizontal cells in computing the gradient. The results are decent with F1-Scores of over 95\% on every cell setup, showing that the system is very robust.

\begin{figure}[htb]
	\begin{center}
		\includegraphics[scale=0.48]{img/result107.jpg}
	    \caption{The overlapping between detection and ground truth car.\label{img:result107}}
    \end{center}
\end{figure}
